---

## ğŸ™ï¸ **YouTube Presentation Script â€” â€œSemantic Search + RAG Application (Bedrock + OpenSearch)â€**

---

### ğŸ§© **#1. What is Semantic Search?**

> â€œLetâ€™s start with the core concept â€” Semantic Search.â€

Traditional search just matches **keywords**.
But **Semantic Search** understands **meaning** â€” it finds results that are *conceptually* similar, not just textually similar.

Hereâ€™s how it works:

> ğŸ” Input query
> â¬‡ï¸
> ğŸ§  Encode to vector
> â¬‡ï¸
> ğŸ“Š Compare similarity with data vectors
> â¬‡ï¸
> ğŸ¯ Return the most semantically relevant results

So when you ask a question, instead of matching exact words, it finds the content that **means the same thing** â€” thatâ€™s the magic of embeddings.

---

### ğŸ§  **#2. What is RAG (Retrieval-Augmented Generation)?**

> â€œNow, letâ€™s talk about RAG â€” short for Retrieval-Augmented Generation.â€

Imagine you have a smart assistant that can generate text, like ChatGPT â€”
but it doesnâ€™t know about your private documents or recent data.

RAG fixes that.

Hereâ€™s the flow:

> ğŸ’¬ Input query
> â¬‡ï¸
> ğŸ“š Retrieve relevant information
> â¬‡ï¸
> ğŸ§  Feed into a language model
> â¬‡ï¸
> âœ¨ Generate an answer based on data and context

So, RAG combines **retrieval** (searching for facts)
and **generation** (producing natural language responses).

â¡ï¸ And remember: **Semantic Search** is the *retrieval brain* of RAG â€”
it helps the model find and use **relevant knowledge**,
so answers become factual and context-aware instead of vague.

---

### ğŸš€ **#3. What We Will Build**

> â€œLetâ€™s see what weâ€™re going to build together.â€

Our application has two main functions:

1. **Ingest** â€” upload documents â†’ split into chunks â†’ generate embeddings â†’ store in OpenSearch
2. **Query** â€” user sends a query â†’ system retrieves semantic context â†’ sends to Bedrock â†’ generates an answer

Weâ€™ll build this entire flow using **AWS services**.

Our **system overview** includes:

* **S3** for document storage
* **Bedrock** for embeddings and LLMs
* **OpenSearch** for vector search
* **Lambda** for workflow logic
* **API Gateway** as the public entry point

This is the architecture behind our *â€œSemantic Search + RAG Application.â€*

---

### âš™ï¸ **#4. Workflow in Action**

> â€œLetâ€™s see how the workflow actually runs.â€

#### **Step 1: Ingest Documents**

We start from document upload â€” thatâ€™s handled through **API Gateway**,
which triggers a series of **Lambda functions** and **Step Functions**:

Stage 1:

> Upload â†’ Extract â†’ Chunk â†’ Generate Embedding (via Bedrock) â†’ Store to OpenSearch

This is our **Document Ingestion Pipeline**.
It transforms your raw files into searchable semantic vectors.

#### **Step 2: Query + RAG**

When a user sends a query:

> Query â†’ Embedding â†’ Search in OpenSearch â†’ Retrieve top results â†’ Send to Bedrock LLM â†’ Generate response

Thatâ€™s our **RAG Query Pipeline** â€”
it finds the best-matching context and uses the LLM to produce an answer thatâ€™s both *relevant* and *informative*.

**Main AWS Services in Action:**

| Service       | Role               | Detail                           |
| ------------- | ------------------ | -------------------------------- |
| Bedrock (LLM) | Generate responses | Use Claude or Titan models       |
| OpenSearch    | Store embeddings   | Vector index + cosine similarity |
| Lambda        | Processing logic   | Orchestrates workflow steps      |
| S3            | Document source    | Raw data for ingestion           |
| API Gateway   | Entry point        | Connects client and Lambda       |

---

### ğŸ’» **#5. Setup the App**

> â€œNow letâ€™s walk through the setup, step by step.â€

**1. Create an S3 bucket**

* Name it: `semantic-search-input`

**2. Connect the Bedrock embedding model**

* Use: `amazon.titan-embed-text-v2:0`

**3. Connect the Bedrock LLM**

* Start with: `anthropic.claude-v2`
* Then upgrade to: `anthropic.claude-3-5-haiku-20241022-v1:0`

**4. Create a DynamoDB table**

* Name: `semantic-search`

**5. Create an SNS Topic**

* Topic: `semantic-search-topic`
* Subscription: `semantic-search-subscription`

**6. Set up OpenSearch domain and index**

Example index mapping:

```json
PUT semantic-index
{
  "settings": {
    "index.knn": true
  },
  "mappings": {
    "properties": {
      "embedding": { "type": "knn_vector", "dimension": 1024 },
      "text": { "type": "text" },
      "metadata": { "type": "object" }
    }
  }
}
```

**7. Deploy Lambda functions**

* Ingest pipeline:
  `ValidateS3Event`, `GetObjectMetadata`, `ExtractAndChunk`,
  `GenerateEmbedding`, `IndexChunkToOpenSearch`,
  `FinalizeIngest`, `FinalizeNoChunks`

* Query pipeline:
  `Ingest`, `QueryHandler`

**8. Assign IAM Roles and Policies**
Each Lambda gets its own Role & Policy â€”
for example, `ValidateS3EventRole`, `GenerateEmbeddingPolicy`, and so on.

**9. Set up Step Functions**

* Name: `semantic-search`
  â†’ This orchestrates all the ingest steps in sequence.

**10. Configure EventBridge**

* Rule: `S3-StepFunctions`
  â†’ Automatically triggers your Step Function whenever a new file is uploaded.

**11. API Gateway**

* Name: `semantic-search`
* Methods: `/ingest`, `/query`

And thatâ€™s it â€” you now have a full **RAG application pipeline**,
from uploading documents to generating context-aware answers!

---

### ğŸ¯ **Conclusion**

> â€œSo to summarizeâ€¦â€

We combined **Semantic Search** and **RAG**, powered by **Amazon Bedrock + OpenSearch**,
to create a system that can:

* Understand your data,
* Retrieve the most relevant information,
* And generate precise, contextual responses.

This architecture is **scalable**, **serverless**, and **AI-ready** â€”
perfect for enterprise knowledge systems or intelligent chatbots.


---

Báº¡n cÃ³ muá»‘n tÃ´i viáº¿t **pháº§n má»Ÿ Ä‘áº§u vÃ  káº¿t káº¿t clip (intro/outro)** theo phong cÃ¡ch YouTube chuyÃªn nghiá»‡p (vÃ­ dá»¥: â€œWelcome to my channel â€” today weâ€™ll buildâ€¦â€ vÃ  â€œIf you enjoyed this demoâ€¦â€)?
TÃ´i cÃ³ thá»ƒ thÃªm pháº§n Ä‘Ã³ Ä‘á»ƒ báº¡n Ä‘á»c tá»± nhiÃªn hÆ¡n khi ghi hÃ¬nh.
